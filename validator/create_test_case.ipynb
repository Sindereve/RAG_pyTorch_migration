{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f757775",
   "metadata": {},
   "source": [
    "# Создание словаря с тестами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec31b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08545822",
   "metadata": {},
   "source": [
    "## torch.six() ([#94709](https://github.com/pytorch/pytorch/pull/94709))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efdfd6",
   "metadata": {},
   "source": [
    "Модуль для совместиости Python 2 и 3.x был удалён "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426be544",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['torch._six'] = \"import torch._six\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4e4aa",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef4db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._six"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ddcde",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93440f9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._six'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_six\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch._six'"
     ]
    }
   ],
   "source": [
    "import torch._six"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75640f",
   "metadata": {},
   "source": [
    "## torch.qr() ([#69857](https://github.com/pytorch/pytorch/pull/69857))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b09cc6",
   "metadata": {},
   "source": [
    "Разложение QR на множетели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c91062",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['torch.qr'] = \"\"\"\n",
    "import torch\n",
    "result = torch.qr(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd70c40",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77010bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.0.0\n",
      "(tensor([[-0.3162, -0.9487],\n",
      "        [-0.9487,  0.3162]]), tensor([[-3.1623, -4.4272],\n",
      "        [ 0.0000, -0.6325]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch',torch.__version__)\n",
    "\n",
    "result = torch.qr(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79405763",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a94c21",
   "metadata": {},
   "source": [
    "Работает, но предупреждает, что метод устарел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188e9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.7.0+cpu\n",
      "torch.return_types.qr(\n",
      "Q=tensor([[-0.3162, -0.9487],\n",
      "        [-0.9487,  0.3162]]),\n",
      "R=tensor([[-3.1623, -4.4272],\n",
      "        [ 0.0000, -0.6325]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinde\\AppData\\Local\\Temp\\ipykernel_20888\\3439701984.py:4: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2493.)\n",
      "  result = torch.qr(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch',torch.__version__)\n",
    "\n",
    "result = torch.qr(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02733640",
   "metadata": {},
   "source": [
    "## cholesky() ([#69857](https://github.com/pytorch/pytorch/pull/69857))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e245b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"torch.cholesky_solve\"] = \"\"\"\n",
    "import torch\n",
    "U = torch.tensor([[1.0, 2.0], [0.0, 1.0]])\n",
    "result = torch.cholesky(U)\n",
    "print(result)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15ab3d",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c346f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "U = torch.tensor([[1.0, 2.0], [0.0, 1.0]])\n",
    "result = torch.cholesky(U)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc02dc4",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da5807",
   "metadata": {},
   "source": [
    "Работает, но выдаёт предупреждение о том что функция устарела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b55e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinde\\AppData\\Local\\Temp\\ipykernel_7496\\3599700712.py:5: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).mH\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1767.)\n",
      "  result = torch.cholesky(U)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "U = torch.tensor([[1.0, 2.0], [0.0, 1.0]])\n",
    "result = torch.cholesky(U)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b838b4",
   "metadata": {},
   "source": [
    "## torch.potri() ([#22841](https://github.com/pytorch/pytorch/pull/22841))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ecdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"torch.potri\"] = \"\"\"\n",
    "import torch\n",
    "a = torch.tensor([[1.0, 0.2], [0.0, 1.9]])\n",
    "u = torch.cholesky(a)\n",
    "result = torch.potri(u)\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5356a1",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801b772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "tensor([[1.0000, -0.0000],\n",
      "        [-0.0000, 0.5263]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "a = torch.tensor([[1.0, 0.2], [0.0, 1.9]])\n",
    "u = torch.cholesky(a)\n",
    "result = torch.potri(u)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f3253",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinde\\AppData\\Local\\Temp\\ipykernel_18612\\2721103264.py:4: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).mH\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1767.)\n",
      "  u = torch.cholesky(a)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'potri'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m a = torch.tensor([[\u001b[32m1.0\u001b[39m, \u001b[32m0.2\u001b[39m], [\u001b[32m0.0\u001b[39m, \u001b[32m1.9\u001b[39m]])\n\u001b[32m      4\u001b[39m u = torch.cholesky(a)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpotri\u001b[49m(u)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'potri'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "a = torch.tensor([[1.0, 0.2], [0.0, 1.9]])\n",
    "u = torch.cholesky(a)\n",
    "result = torch.potri(u)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ab7b7",
   "metadata": {},
   "source": [
    "## torch.trtrs ([#22841](https://github.com/pytorch/pytorch/pull/22841))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffefa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"torch.trtrs\"] = \"\"\"\n",
    "import torch\n",
    "\n",
    "a = torch.triu(torch.randn(3,3))\n",
    "b = torch.randn(3,1)\n",
    "solution, cloned_a = torch.trtrs(b, a)\n",
    "print(solution) \n",
    "print(cloned_a)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac466f56",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b144240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "tensor([[-1.4166],\n",
      "        [-0.5993],\n",
      "        [ 1.2320]])\n",
      "tensor([[ 1.7628,  0.3828,  1.4104],\n",
      "        [ 0.0000,  0.9611, -0.5843],\n",
      "        [ 0.0000,  0.0000,  1.2571]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "a = torch.triu(torch.randn(3,3))\n",
    "b = torch.randn(3,1)\n",
    "solution, cloned_a = torch.trtrs(b, a)\n",
    "print(solution) \n",
    "print(cloned_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dda1b",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5d3b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'trtrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m a = torch.triu(torch.randn(\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m))\n\u001b[32m      5\u001b[39m b = torch.randn(\u001b[32m3\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m solution, cloned_a = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrtrs\u001b[49m(b, a)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(solution) \n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(cloned_a)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'trtrs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "a = torch.triu(torch.randn(3,3))\n",
    "b = torch.randn(3,1)\n",
    "solution, cloned_a = torch.trtrs(b, a)\n",
    "print(solution) \n",
    "print(cloned_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3c8ed",
   "metadata": {},
   "source": [
    "## zero_grad() в оптимизаторах ([#92731](https://github.com/pytorch/pytorch/pull/92731))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6792c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"zero_grad()\"] = \"\"\"import torch\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "x = torch.tensor([[1.0]])\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for param in model.parameters():\n",
    "    # добавим строчку с ошибкой для новых версий\n",
    "    param.grad + 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a93ad",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ee970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "tensor([[0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "x = torch.tensor([[1.0]])\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73eb0e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db9293",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd40bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "x = torch.tensor([[1.0]])\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40f183c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53749281",
   "metadata": {},
   "source": [
    "## torch.stft() ([#86724](https://github.com/pytorch/pytorch/pull/86724))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a059d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"torch.stft()\"] = \"\"\"import torch  \n",
    "signal = torch.randn(100)  \n",
    "stft = torch.stft(signal, n_fft=16) \n",
    "print(stft.dtype)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11517b5e",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761454a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "signal = torch.randn(100)  \n",
    "stft = torch.stft(signal, n_fft=16) \n",
    "print(stft.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d6abc",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b84b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft requires the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPyTorch:\u001b[39m\u001b[33m\"\u001b[39m, torch.__version__)\n\u001b[32m      4\u001b[39m signal = torch.randn(\u001b[32m100\u001b[39m)  \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m stft = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(stft.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\functional.py:730\u001b[39m, in \u001b[36mstft\u001b[39m\u001b[34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex, align_to_window)\u001b[39m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28minput\u001b[39m = F.pad(\u001b[38;5;28minput\u001b[39m.view(extended_shape), [pad, pad], pad_mode)\n\u001b[32m    729\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.view(\u001b[38;5;28minput\u001b[39m.shape[-signal_dim:])\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43malign_to_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: stft requires the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release."
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "signal = torch.randn(100)  \n",
    "stft = torch.stft(signal, n_fft=16) \n",
    "print(stft.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa912966",
   "metadata": {},
   "source": [
    "## torch.nn.functional.scaled_dot_product_attention() ([New Features](https://newreleases.io/project/github/pytorch/pytorch/release/v2.0.0#:~:text=Add%20torch.nn.functional.scaled_dot_product_attention()%20to%20allow%20writing%20fast%20Transformer%2Dlike%20functions%20and%20use%20it%20to%20speed%20up%20nn.Transformer()%20(%20%2391362%2C%20%2391066%2C%20%2390413%2C%20%2387312%2C%20%2394008%2C%20%2389470%2C%20%2390776%2C%20%2392189)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9261f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"torch.nn.functional.scaled_dot_product_attention()\"] = \"\"\"\n",
    "import torch  \n",
    "q = torch.tensor([[0.1, 0.2, 0.3, 0.3, 0.5]]) \n",
    "k = torch.tensor([[0.7, 0.3, 0.7, 0.2, 0.9]]) \n",
    "v = torch.tensor([[0.2, 0.4, 0.4, 0.75, 0.1]]) \n",
    "attn = (q @ k.transpose(-2,-1)) / (10**0.5)  \n",
    "attn = torch.softmax(attn, dim=-1) @ v\n",
    "print(attn)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe82160",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fce7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "tensor([[0.2000, 0.4000, 0.4000, 0.7500, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "q = torch.tensor([[0.1, 0.2, 0.3, 0.3, 0.5]]) \n",
    "k = torch.tensor([[0.7, 0.3, 0.7, 0.2, 0.9]]) \n",
    "v = torch.tensor([[0.2, 0.4, 0.4, 0.75, 0.1]]) \n",
    "attn = (q @ k.transpose(-2,-1)) / (10**0.5)  \n",
    "attn = torch.softmax(attn, dim=-1) @ v\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0895cb4",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ec16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n",
      "tensor([[0.2000, 0.4000, 0.4000, 0.7500, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "q = torch.tensor([[0.1, 0.2, 0.3, 0.3, 0.5]]) \n",
    "k = torch.tensor([[0.7, 0.3, 0.7, 0.2, 0.9]]) \n",
    "v = torch.tensor([[0.2, 0.4, 0.4, 0.75, 0.1]]) \n",
    "attn = (q @ k.transpose(-2,-1)) / (10**0.5)  \n",
    "attn = torch.softmax(attn, dim=-1) @ v\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e84eee",
   "metadata": {},
   "source": [
    "Код работает в новых версиях, но его можно изменить с помощью новой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205ca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.4000, 0.4000, 0.7500, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0.1, 0.2, 0.3, 0.3, 0.5]]) \n",
    "k = torch.tensor([[0.7, 0.3, 0.7, 0.2, 0.9]]) \n",
    "v = torch.tensor([[0.2, 0.4, 0.4, 0.75, 0.1]]) \n",
    "attn = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fa61d",
   "metadata": {},
   "source": [
    "## Пользовательская EmbedModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda88b08",
   "metadata": {},
   "source": [
    "!Подробное описание кода ниже в версиях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b91612",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[\"EmbedModel\"] = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)\n",
    "        A = torch.randn(embeds.size(1), embeds.size(1))\n",
    "        b = embeds.t()\n",
    "        # torch.gesv заменена в новых версиях.(решает систему Ax=b)\n",
    "        # в новой версии используется torch.solve(b, A)[0]\n",
    "        solution = torch.gesv(b, A)[0]\n",
    "        embeds = solution.t()\n",
    "        return embeds\n",
    "\n",
    "vocab_size = 100  \n",
    "embed_dim = 5  \n",
    "epochs = 100  \n",
    "learning_rate = 0.01  \n",
    "model = EmbeddingModel(vocab_size, embed_dim)  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "criterion = nn.MSELoss()  \n",
    "x_train = torch.tensor([1, 2, 3])  \n",
    "y_train = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                        [0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "                        [0.3, 0.4, 0.5, 0.6, 0.7]])  \n",
    "for epoch in range(epochs):  \n",
    "    model.train()  \n",
    "    outputs = model(x_train)  \n",
    "    loss = criterion(outputs, y_train)  \n",
    "    optimizer.zero_grad()  # в новых версиях изменяется на None, в старых приводилось к 0\n",
    "    loss.backward()  \n",
    "    optimizer.step()  \n",
    "\n",
    "print('End train model')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ba0ac",
   "metadata": {},
   "source": [
    "### v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fb9b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.0\n",
      "End train model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)\n",
    "        A = torch.randn(embeds.size(1), embeds.size(1))\n",
    "        b = embeds.t()\n",
    "        # torch.gesv заменена в новых версиях.(решает систему Ax=b)\n",
    "        # в новой версии используется torch.solve(b, A)[0]\n",
    "        solution = torch.gesv(b, A)[0]\n",
    "        embeds = solution.t()\n",
    "        return embeds\n",
    "\n",
    "vocab_size = 100  \n",
    "embed_dim = 5  \n",
    "epochs = 100  \n",
    "learning_rate = 0.01  \n",
    "model = EmbeddingModel(vocab_size, embed_dim)  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "criterion = nn.MSELoss()  \n",
    "x_train = torch.tensor([1, 2, 3])  \n",
    "y_train = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                        [0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "                        [0.3, 0.4, 0.5, 0.6, 0.7]])  \n",
    "for epoch in range(epochs):  \n",
    "    model.train()  \n",
    "    outputs = model(x_train)  \n",
    "    loss = criterion(outputs, y_train)  \n",
    "    optimizer.zero_grad()  # в новых версиях изменяется на None, в старых приводилось к 0\n",
    "    loss.backward()  \n",
    "    optimizer.step()  \n",
    "\n",
    "print('End train model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720e704",
   "metadata": {},
   "source": [
    "### v2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aecbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'gesv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \n\u001b[32m     32\u001b[39m     model.train()  \n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     34\u001b[39m     loss = criterion(outputs, y_train)  \n\u001b[32m     35\u001b[39m     optimizer.zero_grad()  \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mEmbeddingModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m b = embeds.t()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# torch.gesv заменена в новых версиях.(решает систему Ax=b)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# в новой версии используется torch.solve(b, A)[0]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m solution = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgesv\u001b[49m(b, A)[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m embeds = solution.t()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Miniconda3\\envs\\torch270\\Lib\\site-packages\\torch\\__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'gesv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print('PyTorch:', torch.__version__)\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)\n",
    "        A = torch.randn(embeds.size(1), embeds.size(1))\n",
    "        b = embeds.t()\n",
    "        solution = torch.gesv(b, A)[0]\n",
    "        embeds = solution.t()\n",
    "        return embeds\n",
    "\n",
    "vocab_size = 100  \n",
    "embed_dim = 5  \n",
    "epochs = 100  \n",
    "learning_rate = 0.01  \n",
    "model = EmbeddingModel(vocab_size, embed_dim)  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "criterion = nn.MSELoss()  \n",
    "x_train = torch.tensor([1, 2, 3])  \n",
    "y_train = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                        [0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "                        [0.3, 0.4, 0.5, 0.6, 0.7]]) \n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    model.train()  \n",
    "    outputs = model(x_train)  \n",
    "    loss = criterion(outputs, y_train)  \n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()  \n",
    "    optimizer.step()  \n",
    "\n",
    "print('End train model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b78fd",
   "metadata": {},
   "source": [
    "## Созранение словаря с тестами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53230def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch._six': 'import torch._six',\n",
       " 'torch.qr': '\\nimport torch\\nresult = torch.qr(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))\\nprint(result)\\n',\n",
       " 'torch.cholesky_solve': '\\nimport torch\\nU = torch.tensor([[1.0, 2.0], [0.0, 1.0]])\\nresult = torch.cholesky(U)\\nprint(result)',\n",
       " 'torch.potri': '\\nimport torch\\na = torch.tensor([[1.0, 0.2], [0.0, 1.9]])\\nu = torch.cholesky(a)\\nresult = torch.potri(u)\\nprint(result)\\n',\n",
       " 'torch.trtrs': '\\nimport torch\\n\\na = torch.triu(torch.randn(3,3))\\nb = torch.randn(3,1)\\nsolution, cloned_a = torch.trtrs(b, a)\\nprint(solution) \\nprint(cloned_a)\\n',\n",
       " 'zero_grad()': 'import torch\\nmodel = torch.nn.Linear(1, 1)\\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\\n\\nx = torch.tensor([[1.0]])\\ny = model(x)\\ny.sum().backward()\\n\\noptimizer.zero_grad()\\nfor param in model.parameters():\\n    # добавим строчку с ошибкой для новых версий\\n    param.grad + 1\\n',\n",
       " 'torch.stft()': 'import torch  \\nsignal = torch.randn(100)  \\nstft = torch.stft(signal, n_fft=16) \\nprint(stft.dtype)',\n",
       " 'torch.nn.functional.scaled_dot_product_attention()': '\\nimport torch  \\nq = torch.tensor([[0.1, 0.2, 0.3, 0.3, 0.5]]) \\nk = torch.tensor([[0.7, 0.3, 0.7, 0.2, 0.9]]) \\nv = torch.tensor([[0.2, 0.4, 0.4, 0.75, 0.1]]) \\nattn = (q @ k.transpose(-2,-1)) / (10**0.5)  \\nattn = torch.softmax(attn, dim=-1) @ v\\nprint(attn)',\n",
       " 'EmbedModel': \"\\nimport torch\\nimport torch.nn as nn\\n\\nclass EmbeddingModel(nn.Module):\\n    def __init__(self, vocab_size, embed_dim):\\n        super(EmbeddingModel, self).__init__()\\n        self.embed = nn.Embedding(vocab_size, embed_dim)\\n\\n    def forward(self, x):\\n        embeds = self.embed(x)\\n        A = torch.randn(embeds.size(1), embeds.size(1))\\n        b = embeds.t()\\n        # torch.gesv заменена в новых версиях.(решает систему Ax=b)\\n        # в новой версии используется torch.solve(b, A)[0]\\n        solution = torch.gesv(b, A)[0]\\n        embeds = solution.t()\\n        return embeds\\n\\nvocab_size = 100  \\nembed_dim = 5  \\nepochs = 100  \\nlearning_rate = 0.01  \\nmodel = EmbeddingModel(vocab_size, embed_dim)  \\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \\ncriterion = nn.MSELoss()  \\nx_train = torch.tensor([1, 2, 3])  \\ny_train = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], \\n                        [0.2, 0.3, 0.4, 0.5, 0.6], \\n                        [0.3, 0.4, 0.5, 0.6, 0.7]])  \\nfor epoch in range(epochs):  \\n    model.train()  \\n    outputs = model(x_train)  \\n    loss = criterion(outputs, y_train)  \\n    optimizer.zero_grad()  # в новых версиях изменяется на None, в старых приводилось к 0\\n    loss.backward()  \\n    optimizer.step()  \\n\\nprint('End train model')\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50942cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"old_code.json\", \"w\") as file:\n",
    "    json.dump(test_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "migratetorch_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
